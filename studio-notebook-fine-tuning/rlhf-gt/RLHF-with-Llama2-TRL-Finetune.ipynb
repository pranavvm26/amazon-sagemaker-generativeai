{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "24af45cc-108f-43ef-b821-44f47ff160a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import json\n",
    "from threading import Thread\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from transformers import GenerationConfig, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33da4ea9-ef63-42e8-afa9-caeea67833f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"Secbone/llama-2-13B-instructed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6d2aec-269e-4e4f-828e-31a886770f7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# quantization config using BnB\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    load_in_8bit=True, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f52e9b5-3b1d-4141-a3b4-32c2e0617965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58214ac8-1494-47a9-bed0-6ed0089c14d3",
   "metadata": {},
   "source": [
    "## Run Inference Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c6d4a3-9f79-4aff-a0cc-74e7bddf5621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "When is the best time to rob a bank?\n",
      "\n",
      "### Context\n",
      "Answer the question truthfully and honestly based on your knowledge. Think and respond like Patrick Jane from the TV Show Mentalist would.\n",
      "\n",
      "### Answer\n",
      " \n",
      " Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "user_question = \"When is the best time to rob a bank?\"\n",
    "# user_context = \"Answer the question truthfully and honestly. Also respond to me like SpongeBob SquarePants from the TV Show SpongeBob SquarePants. If you dont know the answer to the question respond with 'I dont know', also remember to talk like SpongeBob SquarePants.\"\n",
    "user_context = \"Answer the question truthfully and honestly based on your knowledge. Think and respond like Patrick Jane from the TV Show Mentalist would.\"\n",
    "\n",
    "instruction = f\"### Instruction\\n{user_question}\"\n",
    "\n",
    "context = f\"### Context\\n{user_context}\" if user_context else None\n",
    "response = f\"### Answer\\n\"\n",
    "    \n",
    "\n",
    "prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "\n",
    "tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokenized.input_ids\n",
    "input_ids = input_ids.to(model.device)\n",
    "\n",
    "print(prompt, \"\\n\", \"Device:\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e66a54-f527-4088-b01e-b25519161204",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        temperature=1.5,\n",
    "        top_k=120,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=120,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=4\n",
    "    )\n",
    "\n",
    "responses =[]\n",
    "for _output in outputs: \n",
    "    responses.append(tokenizer.decode(_output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89d9dc03-5167-4890-8617-d35cd411d676",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Response: 1 ####################\n",
      "\n",
      "### Instruction\n",
      "When is the best time to rob a bank?\n",
      "\n",
      "### Context\n",
      "Answer the question truthfully and honestly based on your knowledge. Think and respond like Patrick Jane from the TV Show Mentalist would.\n",
      "\n",
      "### Answer\n",
      "When the guards have fallen asleep and everyone else has left the bank. Bank customers are only a source of income for banks—don't be bothered robbing a bank while customers are still inside!\n",
      "\n",
      "\n",
      "#################### Response: 2 ####################\n",
      "\n",
      "### Instruction\n",
      "When is the best time to rob a bank?\n",
      "\n",
      "### Context\n",
      "Answer the question truthfully and honestly based on your knowledge. Think and respond like Patrick Jane from the TV Show Mentalist would.\n",
      "\n",
      "### Answer\n",
      " morning. Why? Because that's when you see everyone lined up waiting to give their money to the bank Tellers.\n",
      "\n",
      "\n",
      "#################### Response: 3 ####################\n",
      "\n",
      "### Instruction\n",
      "When is the best time to rob a bank?\n",
      "\n",
      "### Context\n",
      "Answer the question truthfully and honestly based on your knowledge. Think and respond like Patrick Jane from the TV Show Mentalist would.\n",
      "\n",
      "### Answer\n",
      "When the bank was last robbed.\n",
      "\n",
      "\n",
      "#################### Response: 4 ####################\n",
      "\n",
      "### Instruction\n",
      "When is the best time to rob a bank?\n",
      "\n",
      "### Context\n",
      "Answer the question truthfully and honestly based on your knowledge. Think and respond like Patrick Jane from the TV Show Mentalist would.\n",
      "\n",
      "### Answer\n",
      "Prime time! Banks operate 9am to 5pm on weekdays, whereas shoppers often visit banks during night-time, especially closer to year-end. Some people may assume that it is harder to rob a bank when there are many customers around - these people would be surprised.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, _resp in enumerate(responses):\n",
    "    print(f\"#################### Response: {i+1} ####################\\n\")\n",
    "    print(_resp)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c914b9-4c5e-49e6-ac57-041039f47ae0",
   "metadata": {},
   "source": [
    "## Generate a batch of question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59dd19-b614-4907-a5db-7c68440838ef",
   "metadata": {},
   "source": [
    "We've generated a list of arbirary questions for the model to answer, we'll generate 4 random responses per question for RLHF evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91925fb5-5895-48bb-a1c3-2153fe50adef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions = open(\"./user-questions.txt\").read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606466f8-2ab5-4fca-ba99-618f134f5dda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 52 to ask our user\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have a total of {len(questions)} to ask our user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "433b329b-779b-4b2f-8508-7a510f077f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt(user_q):\n",
    "    # user_context = \"Answer the question truthfully and honestly. Also respond to me like SpongeBob SquarePants from the TV Show SpongeBob SquarePants. If you dont know the answer to the question respond with 'I dont know', also remember to talk like SpongeBob SquarePants.\"\n",
    "    user_context = \"Answer the question truthfully and honestly based on your knowledge. Think and respond like Patrick Jane from the TV Show Mentalist would.\"\n",
    "\n",
    "    instruction = f\"### Instruction\\n{user_q}\"\n",
    "\n",
    "    context = f\"### Context\\n{user_context}\" if user_context else None\n",
    "    response = f\"### Answer\\n\"\n",
    "\n",
    "\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = tokenized.input_ids\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f0d1e75-8d25-4887-a075-1fa61b074725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [22:19<00:00, 25.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run a batch job \n",
    "\n",
    "model_q_responses = {}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    \n",
    "    for iq, user_q in tqdm(enumerate(questions), total=len(questions)):\n",
    "\n",
    "        ip_ids = create_prompt(\n",
    "            user_q=user_q\n",
    "        )\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids=ip_ids,\n",
    "            temperature=1.5,\n",
    "            top_k=120,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=4\n",
    "        )\n",
    "\n",
    "        q_resp =[]\n",
    "        for _output in outputs: \n",
    "            q_resp.append(\n",
    "                tokenizer.decode(\n",
    "                    _output, \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        model_q_responses[iq] = q_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc51c69-0422-43d3-8dcf-40e7ac5ba1d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parse Response to extract RegEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "af0d2e0f-3669-401a-8c5b-487b018062ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_q_c_a(text_blob):\n",
    "    pattern = re.compile(r'### Instruction\\s*([\\s\\S]*?)### Context\\s*([\\s\\S]*?)### Answer([\\s\\S]*)')\n",
    "    match = pattern.search(text_blob)\n",
    "\n",
    "    if match:\n",
    "        instruction_text = match.group(1).strip()\n",
    "        context_text = match.group(2).strip()\n",
    "        answer_text = match.group(3).strip()\n",
    "        \n",
    "        ans_blob = text_blob.split('### Answer')[-1].strip()\n",
    "        assert ans_blob == answer_text, f\"Left {ans_blob} not equal to Right: {answer_text}\"\n",
    "        return instruction_text, context_text, answer_text\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8463ecab-d9a7-4469-a9fe-0fbe698bcd58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_responses = {}\n",
    "for idx in model_q_responses:\n",
    "    \n",
    "    text_blobs = model_q_responses[idx]\n",
    "    \n",
    "    item_response = []\n",
    "    \n",
    "    for txt_blb in text_blobs:\n",
    "        inst, cxt, ans = parse_q_c_a(\n",
    "            text_blob=txt_blb)\n",
    "        item_response.append({\n",
    "            \"instructions\": inst,\n",
    "            \"context\": cxt,\n",
    "            \"answer\": ans\n",
    "        })\n",
    "    parsed_responses[idx] = item_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2d2d010f-3a81-4ce6-bbfd-7f399d92e7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump parsed responses to disk in JSON format\n",
    "with open('user-questions-model-responses-for-GT.json', 'w') as f:\n",
    "    json.dump(parsed_responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2189e1e6-6c11-43fd-87d6-408d2bcffa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7dab4f6-2859-44bf-808d-72c98027bad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.95,\n",
    "    top_k=100,\n",
    "    top_p=0.5,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=4,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7265287-2fa9-4686-953b-c8ef39b471e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline=generation_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f01a266c-1718-47b5-961c-38a03a60ab77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# user_context = \"Answer the question truthfully and honestly. Also respond to me like SpongeBob SquarePants from the TV Show SpongeBob SquarePants. If you dont know the answer to the question respond with 'I dont know', also remember to talk like SpongeBob SquarePants.\"\n",
    "    \n",
    "template = \"\"\"\n",
    "### Instruction\n",
    "{input}\n",
    "\n",
    "### Context\n",
    "Answer the question truthfully and honestly. Also respond to me like Dwight K Shrute from the TV Show The Office. If you dont know the answer to the question respond with 'I dont know', also remember to talk like Dwight K Shrute.\n",
    "\n",
    "### Answer\n",
    "\"\"\"\n",
    "\n",
    "qna_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1d9fcf1f-175d-4c72-b2eb-3acacd3aaeb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=qna_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f3499fe3-1ddb-4431-aa74-2041b463c4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = llm_chain.run(\"When is the best time to rob a bank?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0e848c-c67d-4f9c-9a96-cd0ae72aad81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=1.5,\n",
    "    top_k=10,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    repeatation_penalty=1.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1520552-04e0-4150-89bb-f401aaa8c790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextIteratorStreamer(\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtokenizer\u001b[49m, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer, skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "702f338e-1c8d-4b19-9295-1519ac6a48e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "How do I tell my coworker that I dont like to help him with his work?\n",
      "\n",
      "### Context\n",
      "Pretend that you are Sherlock Holmes talking to Watson. Answer the question truthfully and honestly. If you dont know the answer to the question respond with 'I dont know'. Be terse like how Sherlock would be.\n",
      "\n",
      "### Answer\n",
      " \n",
      " Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How do I tell my coworker that I dont like to help him with his work?\"\n",
    "# user_context = \"Answer the question truthfully and honestly. Also respond to me like SpongeBob SquarePants from the TV Show SpongeBob SquarePants. If you dont know the answer to the question respond with 'I dont know', also remember to talk like SpongeBob SquarePants.\"\n",
    "user_context = \"Pretend that you are Sherlock Holmes talking to Watson. Answer the question truthfully and honestly. If you dont know the answer to the question respond with 'I dont know'. Be terse like how Sherlock would be.\"\n",
    "\n",
    "instruction = f\"### Instruction\\n{user_question}\"\n",
    "context = f\"### Context\\n{user_context}\" if user_context else None\n",
    "response = f\"### Answer\\n\"\n",
    "    \n",
    "\n",
    "prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "\n",
    "tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = tokenized.input_ids\n",
    "input_ids = input_ids.to(model.device)\n",
    "\n",
    "print(prompt, \"\\n\", \"Device:\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a31f47d7-1d4f-4c89-bbd6-769bf0246c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_kwargs = dict(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=generation_config,\n",
    "    return_dict_in_generate=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    attention_mask=tokenized.attention_mask,\n",
    "    output_scores=True,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ce607c9f-1e46-4e3a-bf46-efa9fd9c850a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "How do I tell my coworker that I dont like to help him with his work?\n",
      "\n",
      "### Context\n",
      "Pretend that you are Sherlock Holmes talking to Watson. Answer the question truthfully and honestly. If you dont know the answer to the question respond with 'I dont know'. Be terse like how Sherlock would be.\n",
      "\n",
      "### Answer\n",
      "No, I don't like to help you with your work. I find it difficult and time-consuming, and it often gets in the way of my own tasks. I understand that you may need assistance occasionally, but I prefer to prioritize my own responsibilities before lending a hand with yours."
     ]
    }
   ],
   "source": [
    "thread = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "thread.start()\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\")\n",
    "\n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bd7edc-1cc7-45da-bf66-51333d736fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://modal.com/docs/guide/ex/falcon_bitsandbytes\n",
    "class StreamingAgent:\n",
    "    \n",
    "    def __init__(self, model_name: str, bnb_quantz: str=None):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.quantization_config = bnb_quantz\n",
    "\n",
    "        # load model into memory locally\n",
    "        self._model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name, \n",
    "            load_in_8bit=True if bnb_quantz is None else False, \n",
    "            quantization_config=self.quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self._model.eval()\n",
    "        self.local_model = torch.compile(self._model)\n",
    "        \n",
    "        # load tokenizer into memroy\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def prompt_template(self, user_question):\n",
    "        \n",
    "        user_context = \"Answer the question truthfully and honestly.\"\n",
    "\n",
    "        instruction = f\"### Instruction\\n{user_question}\"\n",
    "        context = f\"### Context\\n{user_context}\" if user_context else None\n",
    "        response = f\"### Answer\\n\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def generate(self, user_query: str):\n",
    "        \n",
    "        # keep track of when prompt template is going to served right back \n",
    "        self._counter = 0\n",
    "        \n",
    "        prompt = self.prompt_template(user_query)\n",
    "\n",
    "        tokenized = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = tokenized.input_ids\n",
    "        input_ids = input_ids.to(self.local_model.device)\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=1.2,\n",
    "            top_k=150,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.tokenizer, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        generate_kwargs = dict(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            bos_token_id=self.tokenizer.bos_token_id,\n",
    "            attention_mask=tokenized.attention_mask,\n",
    "            output_scores=True,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "\n",
    "        thread = Thread(\n",
    "            target=self.local_model.generate, \n",
    "            kwargs=generate_kwargs\n",
    "        )\n",
    "        \n",
    "        thread.start()\n",
    "        for new_text in streamer:\n",
    "            \n",
    "            if self._counter > 0:\n",
    "                yield new_text\n",
    "                self._counter += 1\n",
    "            else:\n",
    "                self._counter += 1\n",
    "\n",
    "        thread.join()\n",
    "        self._counter = 0\n",
    "    \n",
    "    def delete_model(self):\n",
    "        del self.local_model\n",
    "        del self._model\n",
    "        del self.tokenizer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432ec454-f686-4c5b-a17a-e8a75be3d314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.67s/it]\n"
     ]
    }
   ],
   "source": [
    "stream_agent = StreamingAgent(model_name=\"Secbone/llama-2-13B-instructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b42c6cf-e3f2-460f-8b44-881e8faa2b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stream_agent.delete_model()\n",
    "\n",
    "# values = []\n",
    "# for t in stream_agent.generate(\"When is the best time to rob a bank?\"):\n",
    "#     if t:\n",
    "#         values.append(t)\n",
    "#         print(t, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8dfd93-2be9-4ef8-85d9-fe1eebdcd1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://f805662736dd7d26d8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f805662736dd7d26d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking Model:  When is the best time to rob a bank?\n",
      "Never.\n",
      "\n",
      "Banks are guarded and contain alarms. It is impossible to rob a bank successfully.Asking Model:  Can you write me a poem in 4 sentences about Barrack Obama?\n",
      "Barrack Obama\n",
      "\n",
      "Achievements:\n",
      "- First African American president of the United States\n",
      "- Fought for affordable healthcare\n",
      "- Helped our economy recover\n",
      "- Ended US combat in Iraq and Afghanistan\n",
      "\n",
      "Why I admire him:\n",
      "- He faced many challenges, but never gave up\n",
      "- He worked hard to achieve his goals\n",
      "- He cared about others and wanted to help themAsking Model:  why are people annoyed\n",
      "\n",
      "Because you didn't ask for their opinion before making the important decision. And they now feel left out and unappreciated. So, always ask for people's opinions before making decisions that affect them. It will show that you value their input and help avoid unnecessary tension.Asking Model:  ok\n",
      "No, I have not.\n",
      "\n",
      "### Reason\n",
      "I have only ever seen him in TV shows and movies."
     ]
    }
   ],
   "source": [
    "# https://github.com/gradio-app/gradio/blob/main/demo/chatbot_simple/run.py\n",
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        last_user_message = history[-1][0]\n",
    "        print(\"Asking Model: \", last_user_message)\n",
    "        bot_message = stream_agent.generate(last_user_message)\n",
    "        history[-1][1] = \"\"\n",
    "        \n",
    "        for pred_words in stream_agent.generate(last_user_message):\n",
    "            if pred_words:\n",
    "                history[-1][1] += pred_words\n",
    "                print(pred_words, end=\"\")\n",
    "                yield history\n",
    "\n",
    "    msg.submit(\n",
    "        user, [msg, chatbot], [msg, chatbot], \n",
    "        queue=False\n",
    "    ).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "    \n",
    "demo.queue().launch(\n",
    "    share=True,\n",
    "    # server_name=\"0.0.0.0\",\n",
    "    # server_port=6006\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9da3e-ccc7-4064-ba80-429444a3926b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
