{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize `gpt-oss` model using SageMaker HyperPod recipes and HyperPod on EKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f12537-2283-4850-aa07-b5261c5ffc89",
   "metadata": {},
   "source": [
    "---\n",
    "In this notebook, we use [SageMaker HyperPod recipes](https://github.com/aws/sagemaker-hyperpod-recipes) to fine-tune the GPT-OSS models. Recipes support fine-tuning the following latest released GPT-OSS models,\n",
    "* [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b)\n",
    "* [openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b)\n",
    "\n",
    "In this notebook, we show you how to use the recipes with SageMaker HyperPod on EKS. To run recipes on SageMaker training jobs, see [finetune_gpt_oss_hyperpod_recipes_tj.ipynb](https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/3_distributed_training/models/openai--gpt-oss/finetune_gpt_oss_hyperpod_recipes_tj.ipynb)\n",
    "\n",
    "**What are GPTâ€‘OSS Models?**\n",
    "\n",
    "OpenAI released **gptâ€‘ossâ€‘120b** and **gptâ€‘ossâ€‘20b** on **Augustâ€¯5,â€¯2025**â€”its first openâ€‘weight language models since GPTâ€‘2. These models are provided under the **Apacheâ€¯2.0 license**, enabling both commercial and non-commercial use with full access to the model weights.\n",
    "\n",
    "- **gptâ€‘ossâ€‘120b**  \n",
    "  - ~117â€¯billion parameters, but only ~5.1â€¯billion active per token via Mixtureâ€‘ofâ€‘Experts (MoE) routing  \n",
    "  - 36 layers, 128 experts total, with 4 active per token  \n",
    "  - Supports up to **128â€¯k context length** using denseâ€¯+â€¯sparse attention, grouped multiâ€‘query attention, and RoPE\n",
    "\n",
    "- **gptâ€‘ossâ€‘20b**  \n",
    "  - ~21â€¯billion parameters, ~3.6â€¯billion active per token  \n",
    "  - 24 layers, 32 total experts, with 4 active per token  \n",
    "  - Same efficient attention and contextâ€‘length capabilities as the large variantÂ \n",
    "\n",
    "These models support **chainâ€‘ofâ€‘thought (CoT) reasoning**, structured outputs, and are compatible with the OpenAI Responses API. You can adjust reasoning effort (low/medium/high) with a simple system messageâ€”balancing latency against performance.\n",
    "\n",
    "- **gptâ€‘ossâ€‘120b** matches or exceeds the performance of OpenAIâ€™s proprietary **o4â€‘mini** model on benchmarks such as Codeforces (coding), MMLU and HLE (general reasoning), HealthBench (health), and AIME (competition math).\n",
    "- **gptâ€‘ossâ€‘20b**, despite its smaller size, outperforms **o3â€‘mini** across similar benchmarks, especially in mathematics and coding domains.\n",
    "\n",
    "---\n",
    "\n",
    "## SageMaker HyperPod on EKS\n",
    "This notebook assumes you already have a HyperPod cluster orchestrated by Amazon EKS setup, and you have `ml.p5.48xlarge` instances in your cluster. If you don't have it setup, follow the instructions to setup a cluster:\n",
    "\n",
    "1. Request the following SageMaker quotas on the Service Quotas console:\n",
    "\n",
    "    `P5 instances (ml.p5.48xlarge) for HyperPod clusters (ml.p5.48xlarge for cluster usage): 1`\n",
    "\n",
    "2. Set up a HyperPod EKS cluster, referring to [Amazon SageMaker HyperPod Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks.html). You can use the [New console experience](https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/00-setup/new-console-experience) to create the cluster, or alternatively you can also use the  CloudFormation template provided in the [HyperPod EKS workshop](https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/00-setup/00-workshop-infra-cfn) and follow the instructions to [set up a cluster](https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/01-cluster) and a development environment to access and submit jobs to the cluster.\n",
    "\n",
    "3. Setup an Amazon FSx for Lustre file system for saving and loading data/checkpoints. You can follow the instructions under [Set Up an FSx for Lustre File System](https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/01-cluster/06-fsx-for-lustre) to setup a FSx Lustre volume and associate it with the cluster.\n",
    "\n",
    "Run the remainder of the notebook in an environment that has access to the cluster. For options, see [Set up your environment](https://catalog.workshops.aws/sagemaker-hyperpod-eks/en-US/00-setup/env-setup).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efa65e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3f9c26e",
   "metadata": {},
   "source": [
    "## Prepare your data\n",
    "\n",
    "This section assumes you are running this from an environment that has the `/fsx` volume mounted, using the instructions above. We will pre-process the dataset and save it to the FSx volume that's mounted to all pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2681b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    " \n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-120b\")\n",
    "messages = dataset[0][\"messages\"]\n",
    "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(conversation)\n",
    " \n",
    "def preprocess_function(example):\n",
    "    return tokenizer.apply_chat_template(example['messages'], \n",
    "                                        return_dict=True, \n",
    "                                        padding=\"max_length\", \n",
    "                                        max_length=4096, \n",
    "                                        truncation=True)\n",
    " \n",
    "def label(x):\n",
    "    x[\"labels\"]=np.array(x[\"input_ids\"])\n",
    "    x[\"labels\"][x[\"labels\"]==tokenizer.pad_token_id]=-100\n",
    "    x[\"labels\"]=x[\"labels\"].tolist()\n",
    "    return x\n",
    " \n",
    "dataset = dataset.map(preprocess_function, \n",
    "                      remove_columns=['reasoning_language', \n",
    "                                      'developer', \n",
    "                                      'user', \n",
    "                                      'analysis', \n",
    "                                      'final',\n",
    "                                      'messages'])\n",
    "dataset = dataset.map(label)\n",
    "\n",
    "# for HyperPod, save to mounted FSx volume\n",
    "dataset.save_to_disk(\"/fsx/multilingual_4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7b9c6",
   "metadata": {},
   "source": [
    "## Set up your environment\n",
    "\n",
    "To fine-tune using HyperPod recipes, start by setting up the virtual environment and installing all necessary dependencies to run the job on the EKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a virtual environment\n",
    "!python3 -m venv ${PWD}/venv\n",
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae018d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and set up the HyperPod recipes repo\n",
    "!git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git\n",
    "!cd sagemaker-hyperpod-recipes\n",
    "!pip3 install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d906fd1",
   "metadata": {},
   "source": [
    "You can now use the recipes' `launch_scripts` to submit your training job! Follow the steps below to submit the job:\n",
    "\n",
    "1. In `recipes_collection/cluster/k8s.yaml`, update `persistent_volume_claims` section. It mounts the FSx claim to the `/fsx` directory of each computing pod.\n",
    "```\n",
    "- claimName: fsx-claim    \n",
    "  mountPath: fsx\n",
    "```\n",
    "2. Update the launch script for the GPT-OSS 120B model, available in `launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.sh`, with the `cluster_type` of your HyperPod cluster. In out case, we will update the script with `cluster=k8s` and `cluster_type=k8s` values to the script. Your modified script should look like below:\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "# Original Copyright (c), NVIDIA CORPORATION. Modifications Â© Amazon.com\n",
    "\n",
    "#Users should setup their cluster type in /recipes_collection/config.yaml\n",
    "\n",
    "SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-\"$(pwd)\"}\n",
    "\n",
    "HF_MODEL_NAME_OR_PATH=\"openai/gpt-oss-120b\" # HuggingFace pretrained model name or path\n",
    "\n",
    "TRAIN_DIR=\"/fsx/multilingual_4096\" # Location of training dataset\n",
    "VAL_DIR=\"/fsx/multilingual_4096\" # Location of validation dataset\n",
    "\n",
    "EXP_DIR=\"/fsx/experiment\" # Location to save experiment info including logging, checkpoints, ect\n",
    "HF_ACCESS_TOKEN=\"hf_xxxxxxxx\" # Optional HuggingFace access token\n",
    "\n",
    "HYDRA_FULL_ERROR=1 python3 \"${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py\" \\\n",
    "    recipes=fine-tuning/gpt_oss/hf_gpt_oss_120b_seq4k_gpu_lora \\\n",
    "    container=\"658645717510.dkr.ecr.us-west-2.amazonaws.com/smdistributed-modelparallel:sm-pytorch_gpt_oss_patch_pt-2.7_cuda12.8\" \\\n",
    "    base_results_dir=\"${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results\" \\\n",
    "    recipes.run.name=\"hf-gpt-oss-120b-lora\" \\\n",
    "    cluster=k8s \\ # Imp: add cluster line when running on HP EKS\n",
    "    cluster_type=k8s \\ # Imp: add cluster_type line when running on HP EKS\n",
    "    recipes.exp_manager.exp_dir=\"$EXP_DIR\" \\\n",
    "    recipes.trainer.num_nodes=1 \\\n",
    "    recipes.model.data.train_dir=\"$TRAIN_DIR\" \\\n",
    "    recipes.model.data.val_dir=\"$VAL_DIR\" \\\n",
    "    recipes.model.hf_model_name_or_path=\"$HF_MODEL_NAME_OR_PATH\" \\\n",
    "    recipes.model.hf_access_token=\"$HF_ACCESS_TOKEN\" \\\n",
    "```\n",
    "\n",
    "3. Now, launch the job by running the launcher script:\n",
    "```\n",
    "chmod +x launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.sh \n",
    "bash launcher_scripts/gpt_oss/run_hf_gpt_oss_120b_seq4k_gpu_lora.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1151e3c",
   "metadata": {},
   "source": [
    "## Monitoring the job\n",
    "\n",
    "You can now use `kubectl` commands to monitor your job. Run `kubectl get pods` to see the pod running your job.\n",
    "\n",
    "\n",
    "Once you have the pod name, you can use the `logs` command as shown below to monitor the job:\n",
    "```\n",
    "kubectl logs -f hf-gpt-oss-120b-lora-h2cwd-worker-0\n",
    "```\n",
    "\n",
    "When the status of the pod gets to `Completed`, the final merged model can be found in the experiment directory path we defined in the launcher script under `/fsx/experiment/checkpoints/peft_full/steps_50/final-model`.\n",
    "\n",
    "That's it! Your fine-tuning using HyperPod recipes is now complete! To deploy the model for inference, follow the steps in [finetune_gpt_oss.ipynb](https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/3_distributed_training/models/openai--gpt-oss/finetune_gpt_oss.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24dd2c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
