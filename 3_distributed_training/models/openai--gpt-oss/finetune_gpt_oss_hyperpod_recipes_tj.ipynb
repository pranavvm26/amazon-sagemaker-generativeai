{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60b91a9-7d9c-4c45-be0f-5a224581f644",
   "metadata": {},
   "source": [
    "# ðŸš€ Customize `gpt-oss` model using SageMaker HyperPod recipes and training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f12537-2283-4850-aa07-b5261c5ffc89",
   "metadata": {},
   "source": [
    "---\n",
    "In this notebook, we use [SageMaker HyperPod recipes](https://github.com/aws/sagemaker-hyperpod-recipes) to fine-tune the GPT-OSS models. Recipes support fine-tuning the following latest released GPT-OSS models,\n",
    "* [openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b)\n",
    "* [openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b)\n",
    "\n",
    "In this notebook, we show you how to use the recipes with SageMaker training jobs. To run recipes on SageMaker HyperPod, see [finetune_gpt_oss_hyperpod_recipes_eks.ipynb](https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/3_distributed_training/models/openai--gpt-oss/finetune_gpt_oss_hyperpod_recipes_eks.ipynb)\n",
    "\n",
    "**What are GPTâ€‘OSS Models?**\n",
    "\n",
    "OpenAI released **gptâ€‘ossâ€‘120b** and **gptâ€‘ossâ€‘20b** on **Augustâ€¯5,â€¯2025**â€”its first openâ€‘weight language models since GPTâ€‘2. These models are provided under the **Apacheâ€¯2.0 license**, enabling both commercial and non-commercial use with full access to the model weights.\n",
    "\n",
    "- **gptâ€‘ossâ€‘120b**  \n",
    "  - ~117â€¯billion parameters, but only ~5.1â€¯billion active per token via Mixtureâ€‘ofâ€‘Experts (MoE) routing  \n",
    "  - 36 layers, 128 experts total, with 4 active per token  \n",
    "  - Supports up to **128â€¯k context length** using denseâ€¯+â€¯sparse attention, grouped multiâ€‘query attention, and RoPE\n",
    "\n",
    "- **gptâ€‘ossâ€‘20b**  \n",
    "  - ~21â€¯billion parameters, ~3.6â€¯billion active per token  \n",
    "  - 24 layers, 32 total experts, with 4 active per token  \n",
    "  - Same efficient attention and contextâ€‘length capabilities as the large variantÂ \n",
    "\n",
    "These models support **chainâ€‘ofâ€‘thought (CoT) reasoning**, structured outputs, and are compatible with the OpenAI Responses API. You can adjust reasoning effort (low/medium/high) with a simple system messageâ€”balancing latency against performance.\n",
    "\n",
    "- **gptâ€‘ossâ€‘120b** matches or exceeds the performance of OpenAIâ€™s proprietary **o4â€‘mini** model on benchmarks such as Codeforces (coding), MMLU and HLE (general reasoning), HealthBench (health), and AIME (competition math).\n",
    "- **gptâ€‘ossâ€‘20b**, despite its smaller size, outperforms **o3â€‘mini** across similar benchmarks, especially in mathematics and coding domains.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce40054-610a-4acc-a546-943893f293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker datasets==4.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d18b7a",
   "metadata": {},
   "source": [
    "Make sure you're on the latest version of SageMaker Python SDK (2.251.0 or later) before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791e72e-82b5-4f8e-a7fe-700e8afdeba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b5f4b",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c673a9-5b9f-47e0-92cf-bb97486b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # or provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "default_bucket_prefix = sagemaker_session.default_bucket_prefix\n",
    "default_bucket_prefix_path = \"\"\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    default_bucket_prefix_path = f\"/{default_bucket_prefix}\"\n",
    "\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2bf05-a59f-43f5-ad2a-8279d3ab8f1c",
   "metadata": {},
   "source": [
    "## Data tokenization\n",
    "\n",
    "We now preprocesses the Multilingual-Thinking dataset for fine-tuning using HyperPod recipes:\n",
    "1. Load the dataset from HuggingFace's repository\n",
    "2. Initialize a tokenizer for the GPT model\n",
    "3. Apply chat template formatting to the messages\n",
    "4. Preprocesses the data by:\n",
    "    - Tokenizing the text with max length of 4096 tokens\n",
    "    - Creating labels for the input sequences\n",
    "    - Handling padding tokens by setting them to -100 (ignored in loss calculation)\n",
    "5. Remove unnecessary columns and saves the processed dataset to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd6608-a09f-4521-8ac8-36ec94cc76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
    "messages = dataset[0][\"messages\"]\n",
    "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(conversation)\n",
    "\n",
    "def preprocess_function(example):\n",
    "    return tokenizer.apply_chat_template(example['messages'], return_dict=True, padding=\"max_length\", max_length=4096, truncation=True)\n",
    "\n",
    "def label(x):\n",
    "    x[\"labels\"]=np.array(x[\"input_ids\"])\n",
    "    x[\"labels\"][x[\"labels\"]==tokenizer.pad_token_id]=-100\n",
    "    x[\"labels\"]=x[\"labels\"].tolist()\n",
    "    return x\n",
    "dataset = dataset.map(preprocess_function, remove_columns=['reasoning_language', 'developer', 'user', 'analysis', 'final','messages'])\n",
    "dataset = dataset.map(label)\n",
    "dataset.save_to_disk(\"./multilingual_4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a3efb",
   "metadata": {},
   "source": [
    "Let's upload this pre-processed data to S3 for use by our training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a6564-4a41-4d05-a550-e43784cb2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "def upload_directory(local_dir, bucket_name, s3_prefix=''):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    for root, dirs, files in os.walk(local_dir):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            # Calculate relative path for S3\n",
    "            relative_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_path = os.path.join(s3_prefix, relative_path).replace(\"\\\\\", \"/\")\n",
    "            \n",
    "            print(f\"Uploading {local_path} to {s3_path}\")\n",
    "            s3_client.upload_file(local_path, bucket_name, s3_path)\n",
    "\n",
    "upload_directory('./multilingual_4096/', default_bucket, '/datasets/multilingual_4096')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65e320-cb5c-4cce-8d71-ff7152ac9f95",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "We'll use the Pytorch estimator to spin up the training job. Use the `recipe_overrides` to override any recipe configurations. In this case, since we use SageMaker training jobs, we will update the data locations and model locations to use the `/opt/ml` locations.\n",
    "\n",
    "Note the `hf_model_name_or_path` parameter - this lets our recipe know to use the GPT-OSS 120B model for fine-tuning. You will need a `ml.p5.48xlarge` instance to run the fine-tuning job, so make sure you have sufficient quotas set through Service quotas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c12ae-a6d4-4225-af55-89776bf09cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker,boto3\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "output = os.path.join(f\"s3://{bucket}\", \"output\")\n",
    "\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"results_dir\": \"/opt/ml/model\",\n",
    "    },\n",
    "    \"exp_manager\": {\n",
    "        \"exp_dir\": \"\",\n",
    "        \"explicit_log_dir\": \"/opt/ml/output/tensorboard\",\n",
    "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"data\": {\n",
    "            \"train_dir\": \"/opt/ml/input/data/train\",\n",
    "            \"val_dir\": \"/opt/ml/input/data/val\",\n",
    "        },\n",
    "        \"hf_model_name_or_path\": \"openai/gpt-oss-120b\",\n",
    "    },\n",
    "    \"use_smp_model\": \"false\",\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "  output_path=output,\n",
    "  base_job_name=f\"gpt-oss-recipe\",\n",
    "  role=role,\n",
    "  instance_type=\"ml.p5.48xlarge\",\n",
    "  training_recipe=\"fine-tuning/gpt_oss/hf_gpt_oss_120b_seq4k_gpu_lora\",\n",
    "  recipe_overrides=recipe_overrides,\n",
    "  sagemaker_session=sagemaker_session,\n",
    "  image_uri=\"658645717510.dkr.ecr.us-west-2.amazonaws.com/smdistributed-modelparallel:sm-pytorch_gpt_oss_patch_pt-2.7_cuda12.8\",\n",
    ")\n",
    "\n",
    "estimator.fit(inputs={\"train\": f\"s3://{bucket}/datasets/multilingual_4096/\", \"val\": f\"s3://{bucket}/datasets/multilingual_4096/\"}, wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8fdb5-da51-444b-9b84-f56a88b62c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_data_uri = estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d231be3",
   "metadata": {},
   "source": [
    "That's it, you have fine-tuned the GPT-OSS 120B model on your custom data. To deploy the model for inference, follow the steps in [finetune_gpt_oss.ipynb](https://github.com/aws-samples/amazon-sagemaker-generativeai/blob/main/3_distributed_training/models/openai--gpt-oss/finetune_gpt_oss.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48365995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f961f6e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
