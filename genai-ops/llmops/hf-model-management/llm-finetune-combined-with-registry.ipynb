{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0912c70f-9c8d-480b-9301-db2a99249acc",
   "metadata": {},
   "source": [
    "## Model Management for LoRA Fine-tuned models using Llama2 & Amazon SageMaker (Full Model Copy)\n",
    "\n",
    "In this example notebook, we will walk through an example using LoRA techniques to fine-tune a LLama2 7B model on Amazon SageMaker, and then add the proper model governance using SageMaker Model Registry. While LoRA allows you to store LoRA adapter and base model artifacts separately, this notebook will focus on combining the components and managing a full model copy after finetuning.\n",
    "\n",
    "The example is tested on following kernel and instance types:\n",
    "\n",
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel:</strong> PyTorch 2.0.0 Python 3.10 GPU Optimized, <strong>Instance Type:</strong> ml.g4dn.xlarge\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb034f2-ec0d-45b4-b136-a385c59d157a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43440ba1-410e-4a77-b904-c26130a3b555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq datasets\n",
    "!pip install -Uq transformers==4.31.0\n",
    "!pip install -Uq accelerate==0.21.0\n",
    "!pip install -Uq safetensors>=0.3.1\n",
    "!pip install -Uq botocore\n",
    "!pip install -Uq boto3\n",
    "!pip install -q sagemaker==2.177.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987570d-afff-4189-8d7a-9612fd27b2e8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecca766-8e59-49c5-96e5-11f9558f3f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker.collection import Collection\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d88909-473a-4982-9bbb-94df35b2d618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session =  sagemaker.session.Session() #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "model_collector = Collection(sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69a230-8e94-4b52-9a6c-d5d8bda9fa0a",
   "metadata": {},
   "source": [
    "## Define Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a8ecf-508a-4a58-a77b-d8a52c5887c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_group_for_base = \"llama-2-7b\" # we'll group all llama-2 variants under this collection \n",
    "# define base model name\n",
    "model_id = f\"Mikael110/{model_group_for_base}-guanaco-fp16\" \n",
    "# define a base dataset to finetune this base model\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "\n",
    "# s3 prefix\n",
    "s3_key_prefix = model_id.replace('/', '-')\n",
    "# model collection name\n",
    "model_registry_name = s3_key_prefix\n",
    "\n",
    "model_group_for_finetune = f\"{model_group_for_base}-{dataset_name.split('/')[-1]}\" # all fine tune variant will be base name + dataset name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8829bd7-5a05-4082-a888-f44d08e9ddfc",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "split the data into training and validation and preview the a sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e2521-d25a-4232-bdf0-086babec1b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train[:05%]\")\n",
    "validation_dataset = load_dataset(dataset_name, split=\"train[95%:]\")\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)} | Validation size: {len(validation_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "print(train_dataset[randrange(len(train_dataset))])\n",
    "print(\"\\nValidation sample:\\n\")\n",
    "print(validation_dataset[randrange(len(validation_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7052faf-94a0-46dc-bf1a-ba9db1bd7061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fcd8d-ec61-4655-8ef8-83a28f4b8777",
   "metadata": {},
   "source": [
    "Format the data for instruction fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05a79a-cec9-4e33-8010-fb83fd9c19a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(train_dataset[randrange(len(train_dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5fd33-39ff-47e1-9ed4-a7b5f93f6889",
   "metadata": {},
   "source": [
    "Load the tokenizer for Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3234ac-6f91-4d33-b7ae-e658938cdb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e01ab-751a-4e56-816e-fde2aa421913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "# train\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "# validation\n",
    "validation_dataset = validation_dataset.map(template_dataset, remove_columns=list(validation_dataset.features))\n",
    "# print random sample\n",
    "print(validation_dataset[randint(0, len(validation_dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "\n",
    "# training\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# validation\n",
    "lm_valid_dataset = validation_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(validation_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056f763-39ea-496d-b777-abfbf725e2b4",
   "metadata": {},
   "source": [
    "## Upload dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca8d6f1-c540-460b-b086-eacec5a12b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{default_bucket}/{s3_key_prefix}/dataset/train'\n",
    "lm_train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"saving training dataset to: {training_input_path}\")\n",
    "\n",
    "# save train_dataset to s3\n",
    "validation_input_path = f's3://{default_bucket}/{s3_key_prefix}/dataset/validation'\n",
    "lm_valid_dataset.save_to_disk(validation_input_path)\n",
    "\n",
    "print(f\"saving validation dataset to: {validation_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b7f65-a97f-4488-a5d6-1c095f68ffc9",
   "metadata": {},
   "source": [
    "## Register Base model into Model Registry\n",
    "\n",
    "We are registering the base model into Model registry. This gives a central repository to manage and version base model, so you don't need to duplicate the download from the hub each time you want to experiment or deploy. \n",
    "\n",
    "---\n",
    "download and save the mdoel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad168a9-78eb-4df6-9d8e-5da4388d164c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_save_dir = f\"./base_model/{model_id}\"\n",
    "os.makedirs(base_model_save_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id).save_pretrained(base_model_save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ").save_pretrained(base_model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3dd4d-a177-4ae2-9490-1df69e7572fd",
   "metadata": {},
   "source": [
    "remove model to clear cache memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f850ce-406a-4bba-b8c7-b761cd02a8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "import torch; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0cb85b-cf59-4e93-bbca-dd7d10d72d56",
   "metadata": {},
   "source": [
    "Tar and upload the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f03b5-7a51-485b-9ac7-a52bf4538096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tar_filename = f\"{model_id.replace('/', '-')}.tar.gz\"\n",
    "print(f\"Model tar file name: {model_tar_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab4f07-af5f-4e7d-8a08-6c62d4b7ef90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!cd ./base_model && tar -cvf ./{model_tar_filename} ./{model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39825137-d3e7-4dd2-8a5f-b6851746f773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=f\"./base_model/{model_tar_filename}\",\n",
    "    desired_s3_uri=f's3://{default_bucket}/{s3_key_prefix}/models/base',\n",
    ")\n",
    "print(model_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6391a89-cc12-456b-8d04-f4a5444aa0e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Base Model Package Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06c711-69da-48f8-a9c4-5b4af9717125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Package Group Vars\n",
    "base_package_group_name = name_from_base(model_id.replace('/', '-'))\n",
    "base_package_group_desc = f\"Source: https://huggingface.co/{model_id}\"\n",
    "base_tags = [\n",
    "    { \n",
    "        \"Key\": \"modelType\",\n",
    "        \"Value\": \"BaseModel\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"fineTuned\",\n",
    "        \"Value\": \"False\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"sourceDataset\",\n",
    "        \"Value\": \"None\"\n",
    "    }\n",
    "]\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\" : base_package_group_name,\n",
    "    \"ModelPackageGroupDescription\" : base_package_group_desc,\n",
    "    \"Tags\": base_tags\n",
    "    \n",
    "}\n",
    "create_model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "print(f'Created ModelPackageGroup Arn : {create_model_pacakge_group_response[\"ModelPackageGroupArn\"]}')\n",
    "\n",
    "base_model_pkg_group_name = create_model_pacakge_group_response[\"ModelPackageGroupArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c530fa-eca5-45ee-8647-1a0958966433",
   "metadata": {},
   "source": [
    "### Register the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559aa57-96b8-4632-9453-7c1c6461d1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.28',\n",
    "    pytorch_version='2.0',  \n",
    "    py_version='py310',\n",
    "    model_data=model_data_uri,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a99d6-a54c-4d28-9572-88bc3757d770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_response = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    transform_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    model_package_group_name=base_model_pkg_group_name,\n",
    "    approval_status=\"Approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01fd70-b88e-472a-878b-40e4df2e1acc",
   "metadata": {},
   "source": [
    "### Add Base Model to Model Collection\n",
    "We can associate the base model and the fine tuned model in a model collection. If you get a permission error during creation of collection, please refer to the pre-req or this [AWS documentation to add the IAM polciy](https://docs.aws.amazon.com/sagemaker/latest/dg/modelcollections-permissions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd8657-adcf-49fd-82bb-92af9fe94233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model collection\n",
    "base_collection = model_collector.create(\n",
    "    collection_name=name_from_base(model_group_for_base)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814d0b3-0f30-4d21-a25a-984288e9b103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_response = model_collector.add_model_groups(\n",
    "    collection_name=base_collection[\"Arn\"], \n",
    "    model_groups=[base_model_pkg_group_name]\n",
    ")\n",
    "\n",
    "print(f\"Model collection creation status: {_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d9159-ac44-43d3-86c2-d81ac9cffe27",
   "metadata": {},
   "source": [
    "## Create A Fine Tuning Job\n",
    "\n",
    "We will use a HuggingFace training estimator to fine tune the llama2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1d637-e93a-44c5-9f55-d3b15d695431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc31941-ad73-4d44-9c72-c883edf19f16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.experiments.run import Run\n",
    "\n",
    "# define Training Job Name \n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "job_name = f'huggingface-qlora-{time_suffix}'\n",
    "experiments_name = f\"exp-{model_id.replace('/', '-')}\"\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}\"\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiments_name, \n",
    "    run_name=run_name, \n",
    "    sagemaker_session=sagemaker.Session()\n",
    ") as run:\n",
    "    # create the Estimator\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point='finetune_llm.py',      \n",
    "        source_dir='code',         \n",
    "        instance_type='ml.g5.2xlarge',   \n",
    "        instance_count=1,       \n",
    "        role=role,\n",
    "        base_job_name=job_name,          # the name of the training job\n",
    "        volume_size=300,               \n",
    "        transformers_version='4.28',            \n",
    "        pytorch_version='2.0',             \n",
    "        py_version='py310',           \n",
    "        hyperparameters={\n",
    "            'base_model_group_name': base_package_group_name,\n",
    "            'model_id': model_id,                             \n",
    "            'dataset_path': '/opt/ml/input/data/training',    \n",
    "            'epochs': 1,                                      \n",
    "            'per_device_train_batch_size': 2,                 \n",
    "            'lr': 1e-4,\n",
    "            'merge_weights':True,\n",
    "            'region':region,\n",
    "        },\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    data = {\n",
    "        'training': training_input_path, \n",
    "        'validation': validation_input_path\n",
    "    }\n",
    "    huggingface_estimator.fit(\n",
    "        data, \n",
    "        wait=True\n",
    "    )\n",
    "    \n",
    "    run.log_parameters(data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef5b57-2bdd-4085-b0a1-d8f63aa4430e",
   "metadata": {},
   "source": [
    "## Register the FineTuned model into Model Registry\n",
    "Create model package group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fa0e8-06ab-4e96-8081-79a36a71b132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Package Group Vars\n",
    "ft_package_group_name = name_from_base(f\"{model_id.replace('/', '-')}-finetuned\")\n",
    "ft_package_group_desc = f\"QLoRA for model {model_id}\"\n",
    "ft_tags = [\n",
    "    { \n",
    "        \"Key\": \"modelType\",\n",
    "        \"Value\": \"FineTunedModel\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"fineTuned\",\n",
    "        \"Value\": \"True\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"sourceDataset\",\n",
    "        \"Value\": f\"{dataset_name}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\" : ft_package_group_name,\n",
    "    \"ModelPackageGroupDescription\" : ft_package_group_desc,\n",
    "    \"Tags\": ft_tags\n",
    "    \n",
    "}\n",
    "create_model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "print(f'Created ModelPackageGroup Arn : {create_model_pacakge_group_response[\"ModelPackageGroupArn\"]}')\n",
    "\n",
    "ft_model_pkg_group_name = create_model_pacakge_group_response[\"ModelPackageGroupArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0abd0-1dbf-4175-9da2-7a90a9a2732c",
   "metadata": {},
   "source": [
    "register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f1e84-42b2-4874-8701-ddbc5d0e9afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a31c02-0a5d-4ffe-98f7-4d8eec091df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_package = huggingface_estimator.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\", \n",
    "        \"ml.g5.2xlarge\",\n",
    "        \"ml.g5.12xlarge\",\n",
    "    ],\n",
    "    image_uri = inference_image_uri,\n",
    "    customer_metadata_properties = {\"training-image-uri\": huggingface_estimator.training_image_uri()},  #Store the training image url\n",
    "    model_package_group_name=ft_model_pkg_group_name,\n",
    "    approval_status=\"Approved\"\n",
    ")\n",
    "\n",
    "model_package_arn = model_package.model_package_arn\n",
    "print(\"Model Package ARN : \", model_package_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afe3f5-d5eb-4239-9311-34ae939b929d",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daeb770-41f2-43d1-a2ab-f920d116c8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = f\"{name_from_base(model_group_for_base)}-endpoint\"\n",
    "\n",
    "model_package.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dee280-78fe-41d6-98ee-b6a421ff9335",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Inference\n",
    "\n",
    "Large models such as LLama2 have very high accelerator memory footprint. Thus, a very large input payload or generating a large output can cause out of memory errors. The inference examples below are calibrated such that they will work on the ml.g5.12xlarge instance within the SageMaker response time limit of 60 seconds. If you find that increasing the input length or generation length leads to CUDA Out Of Memory errors, we recommend that you try one of the following solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d6b67-4599-4f12-8553-124512dad5d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "validation_dataset = load_dataset(dataset_name, split=\"train[95%:]\")\n",
    "\n",
    "sample = validation_dataset[randint(0,len(validation_dataset))]\n",
    "\n",
    "instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "response = f\"### Answer\\n\"\n",
    "# join all the parts together\n",
    "prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "\n",
    "    \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84156ff-d47f-4ca7-a837-e525976340ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "data = {\n",
    "    \"text\": prompt,\n",
    "    \"properties\": {\n",
    "        \"min_length\": 10,\n",
    "        \"max_length\": 100,\n",
    "        \"do_sample\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(data),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "outputs = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))['outputs']\n",
    "\n",
    "generated_text = outputs[0]['generated_text']\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4ca2f-aa67-4f8c-bab7-a1d3bfdaa70f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groudtruth = sample['response']\n",
    "print(f\"GroundTruth -> {groudtruth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564e96f-6a86-489a-a937-acfa92334f91",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3514e0e-a861-456e-9cd7-6ebe5f4fd5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed2c00-e958-4c62-8a1a-9fc608a0142a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
