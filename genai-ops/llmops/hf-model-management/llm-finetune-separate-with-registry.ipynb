{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3529419c-859d-4d04-b87f-18ae4e36e532",
   "metadata": {},
   "source": [
    "## Model Management for LoRA Fine-tuned models using Llama2 & Amazon SageMaker (Separate Adapter and Base Models)\n",
    "\n",
    "In this example notebook, we will walk through an example using LoRA techniques to fine-tune a LLama2 7B model on Amazon SageMaker, and then add the proper model governance using SageMaker Model Registry. This notebook focus on saving and managing LoRA adapter and base models seperately. \n",
    "\n",
    "The example is tested on following kernel and instance types:\n",
    "\n",
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel:</strong> PyTorch 2.0.0 Python 3.10 GPU Optimized, <strong>Instance Type:</strong> ml.g4dn.xlarge\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940ae6c-06c9-4fcb-8890-fe47d2064b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50f25a-b0dd-4ba4-9207-9a95237d03c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq datasets\n",
    "!pip install -Uq transformers==4.31.0\n",
    "!pip install -Uq accelerate==0.21.0\n",
    "!pip install -Uq safetensors>=0.3.1\n",
    "!pip install -Uq botocore\n",
    "!pip install -Uq boto3\n",
    "!pip install -q sagemaker==2.177.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y -qq graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c57a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q anytree==2.8.0 pydot==1.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf503e02-49d0-418a-a3b8-a9835aefd99b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358dd2d0-cd4a-4c62-b2e7-6636034f0183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker.collection import Collection\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf85bc0-8f47-42e8-820d-2fdbe7aceacf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session =  sagemaker.session.Session(boto3.session.Session(region_name=\"us-east-1\")) #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "model_collector = Collection(sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b8777-7e66-4ea6-b74b-1a55cea14fe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76a118-1f5f-4c25-8e92-218e1cc5c84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define base model name\n",
    "model_group_for_base = \"llama-2-7b\" # we'll group all llama-2 variants under this collection \n",
    "model_id = f\"Mikael110/{model_group_for_base}-guanaco-fp16\" \n",
    "# define a base dataset to finetune this base model\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "\n",
    "# s3 prefix\n",
    "s3_key_prefix = model_id.replace('/', '-')\n",
    "\n",
    "# base model collection name\n",
    "model_registry_name_base = f\"{s3_key_prefix}-base\"\n",
    "# finetuned model collection name\n",
    "model_registry_name_finetuned = f\"{s3_key_prefix}-finetuned\"\n",
    "model_group_for_finetune = dataset_name.split('/')[-1] # we will group all dataset finetunes to this and attach it back to the parent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d682d0-ed29-44a6-87f5-b13993b39aeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "split the data into training and validation and preview the a sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd444849-0514-43f2-abee-c3b2a52b7b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train[:05%]\")\n",
    "validation_dataset = load_dataset(dataset_name, split=\"train[95%:]\")\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)} | Validation size: {len(validation_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "print(train_dataset[randrange(len(train_dataset))])\n",
    "print(\"\\nValidation sample:\\n\")\n",
    "print(validation_dataset[randrange(len(validation_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42da95-2c97-4ec6-87cd-0cdc7d66bcb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db04f60-47ad-4a06-8319-12103718198d",
   "metadata": {},
   "source": [
    "Format the data for instruction fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d201dd-b9b4-43a5-a333-01a0e067c6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(train_dataset[randrange(len(train_dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bddaa2d-be00-4ee4-aff4-f75e11e83c6b",
   "metadata": {},
   "source": [
    "Load the tokenizer for Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc07b9-0cb5-4103-a58b-c5a6d5044812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3bb03-3d65-4b03-b79e-90b28886627e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "# train\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "# validation\n",
    "validation_dataset = validation_dataset.map(template_dataset, remove_columns=list(validation_dataset.features))\n",
    "# print random sample\n",
    "print(validation_dataset[randint(0, len(validation_dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "\n",
    "# training\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# validation\n",
    "lm_valid_dataset = validation_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(validation_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21ba87-f296-4275-8186-97469a6f5ebd",
   "metadata": {},
   "source": [
    "## Upload dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a2b2b-7f56-46cf-bffe-2e79842242ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{default_bucket}/largelanguagemodels/{model_id}/dataset/train'\n",
    "lm_train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"saving training dataset to: {training_input_path}\")\n",
    "\n",
    "# save train_dataset to s3\n",
    "validation_input_path = f's3://{default_bucket}/largelanguagemodels/{model_id}/dataset/validation'\n",
    "lm_valid_dataset.save_to_disk(validation_input_path)\n",
    "\n",
    "print(f\"saving validation dataset to: {validation_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f4eba-3daf-4487-b28e-f09e158b4335",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Register Base model into Model Registry\n",
    "\n",
    "We are registering the base model into Model registry. This gives a central repository to manage and version base model, so you don't need to duplicate the download from the hub each time you want to experiment or deploy. \n",
    "\n",
    "---\n",
    "download and save the mdoel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823dbae-0315-4bbc-a21f-135f7a663597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_save_dir = f\"./base_model/{model_id}\"\n",
    "os.makedirs(base_model_save_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id).save_pretrained(base_model_save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ").save_pretrained(base_model_save_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bca53e-1c7a-45a0-b097-9c8a6de6e67a",
   "metadata": {},
   "source": [
    "remove model to clear cache memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64aaaa-e569-400b-9c14-48cca6836b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "import torch; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e2ec64-6a29-45c2-ae8a-147b65f26b6e",
   "metadata": {},
   "source": [
    "Tar and upload the model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7041a-ef5f-44dc-886b-42b55a530a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tar_filename = f\"base-model.tar.gz\"\n",
    "print(f\"Model tar file name: {model_tar_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f653d43-6bfc-4d7d-95f6-b7ab86b69d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!cd ./base_model && tar -cvf ./{model_tar_filename} ./{model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47f9fb-7c53-466c-bc7c-0f0c9f557ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=f\"./base_model/{model_tar_filename}\",\n",
    "    desired_s3_uri=f's3://{default_bucket}/largelanguagemodels/{model_id}/models/base',\n",
    ")\n",
    "print(model_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcce7ef-f1b6-4164-a40a-768bb688b5d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a Model Package Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0aef9-a947-4ae4-8cb0-64e4824f4a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Package Group Vars\n",
    "base_package_group_name = name_from_base(model_id.replace('/', '-'))\n",
    "base_package_group_desc = f\"Source: https://huggingface.co/{model_id}\"\n",
    "base_tags = [\n",
    "    { \n",
    "        \"Key\": \"modelType\",\n",
    "        \"Value\": \"BaseModel\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"fineTuned\",\n",
    "        \"Value\": \"False\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"sourceDataset\",\n",
    "        \"Value\": \"None\"\n",
    "    }\n",
    "]\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\" : base_package_group_name,\n",
    "    \"ModelPackageGroupDescription\" : base_package_group_desc,\n",
    "    \"Tags\": base_tags\n",
    "    \n",
    "}\n",
    "create_model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "print(f'Created ModelPackageGroup Arn : {create_model_pacakge_group_response[\"ModelPackageGroupArn\"]}')\n",
    "\n",
    "base_model_pkg_group_name = create_model_pacakge_group_response[\"ModelPackageGroupArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070619c0-5f20-4670-921b-5b39b4d3de86",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Register the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4bdfc-17df-4a8f-9dd6-4ed7ccf1c4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.28',\n",
    "    pytorch_version='2.0',  \n",
    "    py_version='py310',\n",
    "    model_data=model_data_uri,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48572af-00bf-4b90-9fdd-8c467ccbedea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_package = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    transform_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    model_package_group_name=base_model_pkg_group_name,\n",
    "    approval_status=\"Approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f68d9-3f2d-40df-adb4-73db7248098f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add Base Model to Model Collection\n",
    "We can associate the base model and the fine tuned model in a model collection. If you get a permission error during creation of collection, please refer to the pre-req or this [AWS documentation to add the IAM polciy](https://docs.aws.amazon.com/sagemaker/latest/dg/modelcollections-permissions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de687bd-2228-4a33-a80c-35661a2fed37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model collection\n",
    "collection_name = name_from_base(model_group_for_base)\n",
    "base_collection = model_collector.create(\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75053dd6-1c4a-4ae7-b470-8a550435814e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_response = model_collector.add_model_groups(\n",
    "    collection_name=base_collection[\"Arn\"], \n",
    "    model_groups=[base_model_pkg_group_name]\n",
    ")\n",
    "\n",
    "print(f\"Model collection creation status: {_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11564158-f72f-442c-963e-71c56647fe23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create A Fine Tuning Job\n",
    "\n",
    "We will use a HuggingFace training estimator to fine tune the llama2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f4173-0912-48dd-bd03-964346e60cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57df7c5-5cce-4d41-a2d6-dabf67db59d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.experiments.run import Run\n",
    "\n",
    "# define Training Job Name \n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "job_name = f'huggingface-qlora-{time_suffix}'\n",
    "experiments_name = f\"exp-{model_id.replace('/', '-')}\"\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}\"\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiments_name, \n",
    "    run_name=run_name, \n",
    "    sagemaker_session=sagemaker.Session()\n",
    ") as run:\n",
    "    # create the Estimator\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point='finetune_llm.py',      \n",
    "        source_dir='code',         \n",
    "        instance_type='ml.g5.2xlarge',   \n",
    "        instance_count=1,       \n",
    "        role=role,              \n",
    "        volume_size=300,               \n",
    "        transformers_version='4.28',            \n",
    "        pytorch_version='2.0',             \n",
    "        py_version='py310',           \n",
    "        hyperparameters={\n",
    "            'base_model_group_name': base_package_group_name,\n",
    "            'model_id': model_id,                             \n",
    "            'dataset_path': '/opt/ml/input/data/training',    \n",
    "            'epochs': 1,                                      \n",
    "            'per_device_train_batch_size': 2,                 \n",
    "            'lr': 1e-4,\n",
    "            'region': region,\n",
    "        },\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    data = {\n",
    "        'training': training_input_path, \n",
    "        'validation': validation_input_path\n",
    "    }\n",
    "    huggingface_estimator.fit(\n",
    "        data, \n",
    "        wait=True,\n",
    "        job_name=job_name\n",
    "    )\n",
    "    \n",
    "    run.log_parameters(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba930d08-cdf3-4522-b584-3081b5c559c4",
   "metadata": {},
   "source": [
    "## Register the FineTuned model into Model Registry\n",
    "Create model package group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5a384-7a8b-455c-b8b6-ec920a419902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Package Group Vars\n",
    "ft_package_group_name = name_from_base(f\"{model_id.replace('/', '-')}-finetuned-sql\")\n",
    "ft_package_group_desc = f\"QLoRA for model {model_id}\"\n",
    "ft_tags = [\n",
    "    { \n",
    "        \"Key\": \"modelType\",\n",
    "        \"Value\": \"QLoRAModel\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"fineTuned\",\n",
    "        \"Value\": \"True\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"sourceDataset\",\n",
    "        \"Value\": f\"{dataset_name}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\" : ft_package_group_name,\n",
    "    \"ModelPackageGroupDescription\" : ft_package_group_desc,\n",
    "    \"Tags\": ft_tags\n",
    "    \n",
    "}\n",
    "create_model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "print(f'Created ModelPackageGroup Arn : {create_model_pacakge_group_response[\"ModelPackageGroupArn\"]}')\n",
    "\n",
    "ft_model_pkg_group_name = create_model_pacakge_group_response[\"ModelPackageGroupArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1c96e-f9d2-42a0-ac79-2499b2e6bb02",
   "metadata": {},
   "source": [
    "Register a New Model into Fine-Tuned Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08834142-c53a-4b1c-b173-104aed33d257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.28',\n",
    "    pytorch_version='2.0',  \n",
    "    py_version='py310',\n",
    "    model_data=huggingface_estimator.model_data,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ade077-e175-41c8-969d-e0c538b49077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LoRA_package = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    transform_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    model_package_group_name=ft_model_pkg_group_name,\n",
    "    approval_status=\"Approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a70ee4-8766-4170-981d-f2b0abd33be3",
   "metadata": {},
   "source": [
    "Add FineTuned Model to Model Collection with Parent Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_group_for_finetune = name_from_base(model_group_for_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5c010-ff31-414b-8454-0628b9c6d4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model collection for finetuned and link it back to the base\n",
    "finetuned_collection = model_collector.create(\n",
    "    collection_name=_model_group_for_finetune,\n",
    "    parent_collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41762a9e-f4ce-493b-88d4-ce34b488a614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add finetuned model package group to the new finetuned collection\n",
    "_response = model_collector.add_model_groups(\n",
    "    collection_name=_model_group_for_finetune,\n",
    "    model_groups=[ft_model_pkg_group_name]\n",
    ")\n",
    "\n",
    "print(f\"Model collection creation status: {_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af77c6-0345-4269-8306-db48a8ba82fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding Parent (Base) - Child (QLoRA) Model Registry Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c708466-ed6a-4206-b1f6-c8c4405a3928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import OrderedDict\n",
    "from anytree import (\n",
    "    AnyNode as Node, \n",
    "    RenderTree, \n",
    "    DoubleStyle\n",
    ")\n",
    "from anytree.dotexport import RenderTreeGraph\n",
    "\n",
    "\n",
    "def recursively_build_model_tree(\n",
    "    root_model_package_group, \n",
    "    output_dict, \n",
    "    level=0\n",
    "):\n",
    "    \"\"\" Recursively extracts model collections \n",
    "    to build a relationship dictonary \"\"\"\n",
    "    output_dict[root_model_package_group] = []\n",
    "    \n",
    "    model_packages = model_collector.list_collection(root_model_package_group)\n",
    "    \n",
    "    for model_package in model_packages:\n",
    "        if model_package['Type'] == 'Collection':\n",
    "            \n",
    "            output_dict[root_model_package_group].append(\n",
    "                {\n",
    "                    \"package_name\": model_package['Name'],\n",
    "                    \"type\": model_package[\"Type\"]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            recursively_build_model_tree(\n",
    "                model_package['Name'], \n",
    "                output_dict, \n",
    "                level+1\n",
    "            )\n",
    "        elif model_package['Type'] == 'AWS::SageMaker::ModelPackageGroup':\n",
    "            output_dict[root_model_package_group].append(\n",
    "                {\n",
    "                    \"package_name\": model_package['Name'],\n",
    "                    \"type\": model_package[\"Type\"]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def build_tree(raw_data):\n",
    "    \"\"\" Builds a tree using dictionary input \"\"\"\n",
    "    source_dict = {}\n",
    "    for k, values in raw_data.items():\n",
    "        if not any(source_dict):\n",
    "            source_dict[k] = Node(name=k, type_of=\"root\")\n",
    "        for v in values:\n",
    "            source_dict[v['package_name']] = Node(\n",
    "                name=v['package_name'],\n",
    "                type_of=v['type'].split(':')[-1],\n",
    "                parent=source_dict[k]\n",
    "            )\n",
    "    return RenderTree(\n",
    "        source_dict[collection_name], \n",
    "        style=DoubleStyle()\n",
    "    ), source_dict[collection_name]\n",
    "\n",
    "\n",
    "raw_data = recursively_build_model_tree(\n",
    "    root_model_package_group=collection_name, \n",
    "    output_dict=OrderedDict()\n",
    ")\n",
    "\n",
    "_tree, raw_node = build_tree(raw_data=raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b871c17-3ea6-4c19-a21f-39b011ab3d60",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = \"test.jpg\"\n",
    "RenderTreeGraph(raw_node).to_picture(image_path)\n",
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a90315-132f-4471-8859-10c3d0ac062d",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "Step 1: Repack the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696f143-1f21-4f9c-a75e-69f353f311d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp {base_model_package.model_data} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1317ff-065a-4a12-afbb-229de47fc7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -xvf {model_tar_filename} -C ./deepspeed/\n",
    "\n",
    "!mv ./deepspeed/{model_id} ./deepspeed/base\n",
    "\n",
    "!rm -rf ./deepspeed/{model_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57012c77-f1bf-4ebc-aaea-2f4b94c8d3a0",
   "metadata": {},
   "source": [
    "Step 2: we need to download and repackage the LoRA weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24d288-cb37-4a3b-9f74-d9ddac7a59db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp {LoRA_package.model_data} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8c1e6-1c1d-4d36-a065-6da39bed5474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./deepspeed/lora/\n",
    "\n",
    "!tar -xzf model.tar.gz -C ./deepspeed/lora/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b81f9d-0d47-4e49-9816-36cbf8a79037",
   "metadata": {},
   "source": [
    "Create a new model package to deploy. This may take up to 10 minutes to package and upload due to the file size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41ab05-34af-41ee-859a-61e37e73ad12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf `find -type d -name .ipynb_checkpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c79225-5049-4a29-9fbd-065e16c58ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f model.tar.gz\n",
    "!tar czvf model.tar.gz -C deepspeed .\n",
    "s3_code_artifact_deepspeed = sagemaker_session.upload_data(\"model.tar.gz\", default_bucket, f\"{s3_key_prefix}/inference\")\n",
    "print(f\"S3 Code or Model tar for deepspeed uploaded to --- > {s3_code_artifact_deepspeed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e9ea7-8a19-433f-b2c3-f3a2e104aa95",
   "metadata": {},
   "source": [
    "### Define the serving container\n",
    "Here we define the container to use for the model for inference. We will be using SageMaker's Large Model Inference(LMI) container using DeepSpeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f347c8-feb7-4707-9ead-97773e14ac7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=\"0.23.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850aa5c-5bb4-4815-9c39-d4e5a9eb48fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_ds = name_from_base(model_group_for_base)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name_ds,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": s3_code_artifact_deepspeed},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d51ddd-d82b-445f-8f85-d4d3721be00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name_ds}-config\"\n",
    "endpoint_name = f\"{model_name_ds}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name_ds,\n",
    "            \"InstanceType\": \"ml.g5.12xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d806120-fb02-4761-bb6b-2771ea2d8ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28be8aa-2714-4ccf-8b29-af114fc1e364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a89031-da05-4724-a926-a4cafcee3f5b",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Large models such as LLama2 have very high accelerator memory footprint. Thus, a very large input payload or generating a large output can cause out of memory errors. The inference examples below are calibrated such that they will work on the ml.g5.12xlarge instance within the SageMaker response time limit of 60 seconds. If you find that increasing the input length or generation length leads to CUDA Out Of Memory errors, we recommend that you try one of the following solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada084e-98c7-485e-8f62-ee3562f96edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "validation_dataset = load_dataset(dataset_name, split=\"train[95%:]\")\n",
    "\n",
    "sample = validation_dataset[randint(0,len(validation_dataset))]\n",
    "\n",
    "instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "response = f\"### Answer\\n\"\n",
    "# join all the parts together\n",
    "prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "\n",
    "    \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada52f8-d9db-4140-ab40-ce4ec71c185b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "data = {\n",
    "    \"text\": prompt,\n",
    "    \"properties\": {\n",
    "        \"min_length\": 10,\n",
    "        \"max_length\": 100,\n",
    "        \"do_sample\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(data),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "outputs = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))['outputs']\n",
    "\n",
    "generated_text = outputs[0]['generated_text']\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9aa7a6-7ea6-415f-aecd-522d7396809c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "groudtruth = sample['response']\n",
    "print(f\"GroundTruth -> {groudtruth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb14ba-ae7e-46e3-991f-f8d9767d6d85",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb172d-cbd5-42bd-b72b-5e7f6c16cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973088c-fec3-45fd-90c6-978a8cdb5aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
